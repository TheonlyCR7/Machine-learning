# 主成分分析

`Principal Component Analysis`  简称 PCA

* 一个非监督的机器学习算法

* 主要用于数据的**降维**

* 通过降维，可以发现更便于人类理解的特征

* 其他应用：可视化，去噪

## 数据的降维

* 例如将数据从二维降到一维
  
  将所有的点都映射到x 轴 或是 y 轴
  
  还可以将所有点都映射到一条<u>最佳直线</u>上

<img src="D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-02-31-image.png" title="" alt="" width="550">e.png" title="" alt="" width="550">

找到这样的一条直线

映射后，让映射后样本间的间距保持最大（样本间的区别就越明显）

注意：这里是与线性回归是有明显的区别的

* 坐标轴均为特则值，线性回归为特征，样本

* 线性回归为垂直直线映射，而主成分分析法为垂直X轴映射



* 定义样本间间距
  
  使用方差
  
  <img src="D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-08-03-image.png" title="" alt="" width="208">e.png" title="" alt="" width="208">

* 让样本的均值为0
  
  <img title="" src="D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-11-48-image.png" alt="" width="362">
  
  <img src="D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-13-53-image.png" title="" alt="" width="357">e.png" title="" alt="" width="357">
  
  然后基于这条直线为轴 求出轴的方向 W = (W1, W2)
  
  让所有样本，映射到W后，使得 最大
  
  <img title="" src="D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-19-19-image.png" alt="" width="434" data-align="inline">e.png" title="" alt="" width="434">   
  
  映射后，这里X的平均值也为零，所以为：
  
  ![](D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-26-45-image.png)
  
  ![](D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-35-26-image.png)

目标为：求w 使得

![](D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-37-54-image.png)

还可以表示为

![](D:\Github_NOTES\Machine-learning\Machine-learning\img\2020-06-13-11-41-16-image.png)

一个目标函数的最优化问题，使用梯度上升法解决








